{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb570ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session initialized for Week 3!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimePrediction_Week3\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"3g\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session initialized for Week 3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d5395d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WEEK 3: GEOSPATIAL ANALYSIS & HOTSPOT DETECTION\n",
      "Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\n",
      "======================================================================\n",
      "✅ Loaded 1,970,206 records\n",
      "\n",
      "Available columns:\n",
      "['date_ts', 'crime_type', 'latitude', 'longitude', 'community_area', 'temp_mean', 'precipitation', 'wind_speed', 'year', 'month', 'hour', 'dayofweek', 'season', 'per_capita_income']\n",
      "\n",
      "Converting to Pandas for geospatial analysis...\n",
      "✅ Converted 1,970,206 records to Pandas\n",
      "\n",
      "✅ Data ready for geospatial analysis!\n",
      "\n",
      "Sample data:\n",
      "     date_ts               crime_type   latitude  longitude  community_area  \\\n",
      "0 2025-11-09            OTHER OFFENSE  41.765286 -87.577086            43.0   \n",
      "1 2025-11-09  CRIMINAL SEXUAL ASSAULT  41.936336 -87.650710             6.0   \n",
      "2 2025-11-09       DECEPTIVE PRACTICE  41.904817 -87.689930            24.0   \n",
      "3 2025-11-09  CRIMINAL SEXUAL ASSAULT  41.877609 -87.667595            28.0   \n",
      "4 2025-11-09              SEX OFFENSE  41.860902 -87.707037            29.0   \n",
      "\n",
      "   temp_mean  year  month  hour season  \n",
      "0        NaN  2025     11     0   fall  \n",
      "1        NaN  2025     11     0   fall  \n",
      "2        NaN  2025     11     0   fall  \n",
      "3        NaN  2025     11     0   fall  \n",
      "4        NaN  2025     11     0   fall  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"WEEK 3: GEOSPATIAL ANALYSIS & HOTSPOT DETECTION\")\n",
    "print(\"Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Loading the saved data\n",
    "df_geo = spark.read.parquet(\"../data/processed/integrated_crime_data.parquet\")\n",
    "\n",
    "print(f\"✅ Loaded {df_geo.count():,} records\")\n",
    "\n",
    "# Checking column names to confirm\n",
    "print(\"\\nAvailable columns:\")\n",
    "print(df_geo.columns)\n",
    "\n",
    "# Converting to Pandas\n",
    "print(\"\\nConverting to Pandas for geospatial analysis...\")\n",
    "df_pandas = df_geo.select(\n",
    "    \"date_ts\",\n",
    "    \"crime_type\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"community_area\",\n",
    "    \"temp_mean\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"season\"\n",
    ").toPandas()\n",
    "\n",
    "print(f\"✅ Converted {len(df_pandas):,} records to Pandas\")\n",
    "print(\"\\n✅ Data ready for geospatial analysis!\")\n",
    "\n",
    "# sample\n",
    "print(\"\\nSample data:\")\n",
    "print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "H3 HEXAGONAL SPATIAL INDEXING\n",
      "======================================================================\n",
      "h3 version: 4.3.1\n",
      "\n",
      "Generating H3 hexagonal grid indices...\n",
      "✅ Added H3 indices to 1,970,206 records\n",
      "✓ Created 893 hexagonal cells\n",
      "✓ Average crimes per hex: 2206.3\n",
      "\n",
      "Top 10 Crime Hotspot Hexagons:\n",
      "            h3_index  crime_count  lat_center  lon_center\n",
      "224  882664c1a9fffff        30767   41.881964  -87.628032\n",
      "234  882664c1e1fffff        26457   41.894715  -87.625989\n",
      "237  882664c1e7fffff        16714   41.889090  -87.631855\n",
      "235  882664c1e3fffff        14558   41.887668  -87.622796\n",
      "240  882664c1edfffff        10835   41.903049  -87.630709\n",
      "673  882664ceb5fffff        10554   41.755311  -87.560330\n",
      "303  882664c8cbfffff        10138   41.861043  -87.712294\n",
      "383  882664caa7fffff         9543   41.875766  -87.723370\n",
      "551  882664cce1fffff         9426   41.745956  -87.605168\n",
      "248  882664c811fffff         9403   41.877779  -87.745298\n",
      "\n",
      "✅ H3 spatial indexing complete!\n"
     ]
    }
   ],
   "source": [
    "import h3\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"H3 HEXAGONAL SPATIAL INDEXING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Checking h3 version\n",
    "print(f\"h3 version: {h3.__version__}\")\n",
    "\n",
    "# Adding H3 index to each crime (resolution 8 = ~0.46 km² hexagons)\n",
    "print(\"\\nGenerating H3 hexagonal grid indices...\")\n",
    "\n",
    "# Using the correct function based on h3 version\n",
    "def get_h3_index(lat, lon, resolution=8):\n",
    "    try:\n",
    "        # Trying new API (h3 v4+)\n",
    "        return h3.latlng_to_cell(lat, lon, resolution)\n",
    "    except AttributeError:\n",
    "        # Falling back to old API (h3 v3)\n",
    "        return h3.geo_to_h3(lat, lon, resolution)\n",
    "\n",
    "df_pandas['h3_index'] = df_pandas.apply(\n",
    "    lambda row: get_h3_index(row['latitude'], row['longitude'], 8),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"✅ Added H3 indices to {len(df_pandas):,} records\")\n",
    "\n",
    "# Aggregating crimes by hexagon\n",
    "hex_crimes = df_pandas.groupby('h3_index').agg({\n",
    "    'crime_type': 'count',\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "hex_crimes.columns = ['h3_index', 'crime_count', 'lat_center', 'lon_center']\n",
    "\n",
    "print(f\"✓ Created {len(hex_crimes):,} hexagonal cells\")\n",
    "print(f\"✓ Average crimes per hex: {hex_crimes['crime_count'].mean():.1f}\")\n",
    "\n",
    "# Showing top crime hotspot hexagons\n",
    "print(\"\\nTop 10 Crime Hotspot Hexagons:\")\n",
    "print(hex_crimes.nlargest(10, 'crime_count')[['h3_index', 'crime_count', 'lat_center', 'lon_center']])\n",
    "\n",
    "print(\"\\n✅ H3 spatial indexing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6a960d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shapely in /opt/homebrew/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: geopandas in /opt/homebrew/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/homebrew/lib/python3.11/site-packages (from shapely) (2.3.5)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /opt/homebrew/lib/python3.11/site-packages (from geopandas) (0.12.1)\n",
      "Requirement already satisfied: packaging in /Users/kiranghumare/Library/Python/3.11/lib/python/site-packages (from geopandas) (25.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from geopandas) (2.3.3)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in /opt/homebrew/lib/python3.11/site-packages (from geopandas) (3.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kiranghumare/Library/Python/3.11/lib/python/site-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from pyogrio>=0.7.2->geopandas) (2025.11.12)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kiranghumare/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Converting H3 indices to polygon boundaries...\n",
      "✅ Created GeoDataFrame with hex polygons\n"
     ]
    }
   ],
   "source": [
    "%pip install shapely geopandas\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "print(\"\\nConverting H3 indices to polygon boundaries...\")\n",
    "\n",
    "def h3_to_poly(h3_index):\n",
    "    boundary = h3.cell_to_boundary(h3_index)\n",
    "    return Polygon([(p[1], p[0]) for p in boundary])  # (lon, lat)\n",
    "\n",
    "hex_crimes[\"geometry\"] = hex_crimes[\"h3_index\"].apply(h3_to_poly)\n",
    "\n",
    "gdf_hex = gpd.GeoDataFrame(hex_crimes, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "print(\"✅ Created GeoDataFrame with hex polygons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating hexagonal hotspot map...\n",
      "✅ Hexagon hotspot map saved!\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import branca.colormap as cm\n",
    "\n",
    "print(\"\\nCreating hexagonal hotspot map...\")\n",
    "\n",
    "# Creating a Folium map\n",
    "m = folium.Map(location=[41.88, -87.63], zoom_start=10)\n",
    "\n",
    "# Creating color scale based on crime counts\n",
    "max_count = gdf_hex[\"crime_count\"].max()\n",
    "colormap = cm.linear.RdBu_09.scale(0, max_count)\n",
    "\n",
    "# Adding each hexagon as a GeoJson overlay\n",
    "for _, row in gdf_hex.iterrows():\n",
    "    geo_json = folium.GeoJson(\n",
    "        row[\"geometry\"].__geo_interface__,\n",
    "        style_function=lambda feature, count=row[\"crime_count\"]: {\n",
    "            \"fillColor\": colormap(count),\n",
    "            \"color\": \"black\",\n",
    "            \"weight\": 0.5,\n",
    "            \"fillOpacity\": 0.6 \n",
    "        },\n",
    "        tooltip=folium.Tooltip(f\"\"\"\n",
    "            Crimes in Hex: {row['crime_count']}<br>\n",
    "            Lat: {row['lat_center']:.4f}<br>\n",
    "            Lon: {row['lon_center']:.4f}\n",
    "        \"\"\")\n",
    "    )\n",
    "    geo_json.add_to(m)\n",
    "\n",
    "colormap.caption = \"Crime Density per H3 Hexagon\"\n",
    "colormap.add_to(m)\n",
    "\n",
    "# Savig the map\n",
    "m.save(\"../outputs/h3_hex_hotspots.html\")\n",
    "\n",
    "print(\"✅ Hexagon hotspot map saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4326e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DBSCAN CLUSTERING FOR HOTSPOT DETECTION\n",
      "======================================================================\n",
      "\n",
      "Applying DBSCAN clustering...\n",
      "✅ Found 2 crime hotspot clusters\n",
      "✅ Noise points (not in clusters): 8\n",
      "\n",
      "Cluster Statistics:\n",
      "        crime_count                lat_center lon_center\n",
      "                sum     mean count       mean       mean\n",
      "cluster                                                 \n",
      "0           1854912  2095.95   885      41.84     -87.69\n",
      "\n",
      "✅ DBSCAN clustering complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DBSCAN CLUSTERING FOR HOTSPOT DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preparing data for clustering (using hexagon centers)\n",
    "X = hex_crimes[['lat_center', 'lon_center', 'crime_count']].values\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Applying DBSCAN\n",
    "# eps=0.5 (distance threshold), min_samples=5 (minimum cluster size)\n",
    "print(\"\\nApplying DBSCAN clustering...\")\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "hex_crimes['cluster'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Analyzing clusters\n",
    "n_clusters = len(set(hex_crimes['cluster'])) - (1 if -1 in hex_crimes['cluster'] else 0)\n",
    "n_noise = list(hex_crimes['cluster']).count(-1)\n",
    "\n",
    "print(f\"✅ Found {n_clusters} crime hotspot clusters\")\n",
    "print(f\"✅ Noise points (not in clusters): {n_noise}\")\n",
    "\n",
    "# Showing cluster statistics\n",
    "cluster_stats = hex_crimes[hex_crimes['cluster'] != -1].groupby('cluster').agg({\n",
    "    'crime_count': ['sum', 'mean', 'count'],\n",
    "    'lat_center': 'mean',\n",
    "    'lon_center': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nCluster Statistics:\")\n",
    "print(cluster_stats.head(10))\n",
    "\n",
    "print(\"\\n✅ DBSCAN clustering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec564dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING INTERACTIVE CRIME MAP\n",
      "======================================================================\n",
      "Using 10,000 crime points for visualization...\n",
      "✅ Interactive map saved to: ../outputs/chicago_crime_heatmap.html\n",
      "✅ Open this file in a browser to view the interactive map!\n",
      "\n",
      "✅ Interactive crime map created!\n"
     ]
    }
   ],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING INTERACTIVE CRIME MAP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sampling data for visualization (using 50k points for performance)\n",
    "df_sample = df_pandas.sample(n=min(10000, len(df_pandas)), random_state=42)\n",
    "print(f\"Using {len(df_sample):,} crime points for visualization...\")\n",
    "\n",
    "# Creating base map centered on Chicago\n",
    "chicago_center = [41.8781, -87.6298]\n",
    "crime_map = folium.Map(\n",
    "    location=chicago_center,\n",
    "    zoom_start=11,\n",
    "    tiles='OpenStreetMap'\n",
    ")\n",
    "\n",
    "# Adding heatmap layer\n",
    "heat_data = [[row['latitude'], row['longitude']] for idx, row in df_sample.iterrows()]\n",
    "\n",
    "HeatMap(\n",
    "    heat_data,\n",
    "    radius=7,\n",
    "    blur=10,\n",
    "    min_opacity=0.3,\n",
    "    max_zoom=12,\n",
    "    name='Crime Heatmap'\n",
    ").add_to(crime_map)\n",
    "\n",
    "# Adding cluster markers for top hotspots\n",
    "top_clusters = hex_crimes[hex_crimes['cluster'] != -1].nlargest(20, 'crime_count')\n",
    "\n",
    "for idx, row in top_clusters.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat_center'], row['lon_center']],\n",
    "        radius=8,\n",
    "        popup=f\"Cluster {row['cluster']}<br>Crimes: {row['crime_count']}\",\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fillColor='red',\n",
    "        fillOpacity=0.6\n",
    "    ).add_to(crime_map)\n",
    "\n",
    "# Saving map\n",
    "map_path = '../outputs/chicago_crime_heatmap.html'\n",
    "crime_map.save(map_path)\n",
    "\n",
    "print(f\"✅ Interactive map saved to: {map_path}\")\n",
    "print(\"✅ Open this file in a browser to view the interactive map!\")\n",
    "\n",
    "print(\"\\n✅ Interactive crime map created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af663688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KERNEL DENSITY ESTIMATION\n",
      "======================================================================\n",
      "Computing KDE with 10,000 points...\n",
      "✅ KDE plot saved to: ../outputs/crime_density_kde.png\n",
      "\n",
      "✅ Kernel Density Estimation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/kq2bk67n4b1_cd_g8kcxdg0m0000gn/T/ipykernel_14097/2689776648.py:47: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KERNEL DENSITY ESTIMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sampling for KDE (10k points for performance)\n",
    "kde_sample = df_pandas.sample(n=min(10000, len(df_pandas)), random_state=42)\n",
    "\n",
    "print(f\"Computing KDE with {len(kde_sample):,} points...\")\n",
    "\n",
    "# Preparing data\n",
    "x = kde_sample['longitude'].values\n",
    "y = kde_sample['latitude'].values\n",
    "\n",
    "# Creating KDE\n",
    "xy = np.vstack([x, y])\n",
    "kde = gaussian_kde(xy)\n",
    "\n",
    "# Creating grid for density plot\n",
    "lon_min, lon_max = x.min(), x.max()\n",
    "lat_min, lat_max = y.min(), y.max()\n",
    "\n",
    "xx, yy = np.mgrid[lon_min:lon_max:100j, lat_min:lat_max:100j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "density = np.reshape(kde(positions).T, xx.shape)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.imshow(\n",
    "    np.rot90(density),\n",
    "    cmap='hot',\n",
    "    extent=[lon_min, lon_max, lat_min, lat_max],\n",
    "    aspect='auto'\n",
    ")\n",
    "ax.scatter(x, y, c='blue', s=1, alpha=0.1)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Chicago Crime Density Map (Kernel Density Estimation)', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/crime_density_kde.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✅ KDE plot saved to: ../outputs/crime_density_kde.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Kernel Density Estimation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57fb812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: xgboost in /opt/homebrew/lib/python3.11/site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "✅ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Installing using pip\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Installing scikit-learn\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"xgboost\"])\n",
    "\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9dd9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec6638fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Failed to open local file '../data/processed/integrated_crime_data.parquet'. Detail: [errno 21] Is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIsADirectoryError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Saving the data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf_ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/processed/integrated_crime_data.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parquet.py:482\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m impl = get_engine(engine)\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parquet.py:229\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[38;5;28mself\u001b[39m.api.parquet.write_to_dataset(\n\u001b[32m    220\u001b[39m             table,\n\u001b[32m    221\u001b[39m             path_or_handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m             **kwargs,\n\u001b[32m    226\u001b[39m         )\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    228\u001b[39m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/parquet/core.py:1958\u001b[39m, in \u001b[36mwrite_table\u001b[39m\u001b[34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, **kwargs)\u001b[39m\n\u001b[32m   1956\u001b[39m use_int96 = use_deprecated_int96_timestamps\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1958\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1963\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1964\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_page_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_page_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_int96\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1976\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1977\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1978\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1979\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1980\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1981\u001b[39m \u001b[43m            \u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1982\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstore_decimal_as_integer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore_decimal_as_integer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[32m   1984\u001b[39m         writer.write_table(table, row_group_size=row_group_size)\n\u001b[32m   1985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/parquet/core.py:1064\u001b[39m, in \u001b[36mParquetWriter.__init__\u001b[39m\u001b[34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, **options)\u001b[39m\n\u001b[32m   1059\u001b[39m filesystem, path = _resolve_filesystem_and_path(where, filesystem)\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1061\u001b[39m     \u001b[38;5;66;03m# ARROW-10480: do not auto-detect compression.  While\u001b[39;00m\n\u001b[32m   1062\u001b[39m     \u001b[38;5;66;03m# a filename like foo.parquet.gz is nonconforming, it\u001b[39;00m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;66;03m# shouldn't implicitly apply compression.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     sink = \u001b[38;5;28mself\u001b[39m.file_handle = \u001b[43mfilesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_output_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1067\u001b[39m     sink = where\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/_fs.pyx:913\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.open_output_stream\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mIsADirectoryError\u001b[39m: [Errno 21] Failed to open local file '../data/processed/integrated_crime_data.parquet'. Detail: [errno 21] Is a directory"
     ]
    }
   ],
   "source": [
    "# Saving the data\n",
    "df_ml.to_parquet(\"../data/processed/integrated_crime_data.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
